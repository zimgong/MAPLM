{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x757257ff29b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm, trange\n",
    "import random\n",
    "from torch import optim\n",
    "\n",
    "import wandb\n",
    "\n",
    "# Helpful for computing cosine similarity--Note that this is NOT a similarity!\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# We'll use this to save our models\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import pickle\n",
    "\n",
    "#\n",
    "# IMPORTANT NOTE: Always set your random seeds when dealing with stochastic\n",
    "# algorithms as it lets your bugs be reproducible and (more importantly) it lets\n",
    "# your results be reproducible by others.\n",
    "#\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Word2Vec import RandomNumberGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a class to hold the data\n",
    "\n",
    "Before we get to training word2vec, we'll need to process the corpus into some representation. The `Corpus` class will handle much of the functionality for corpus reading and keeping track of which word types belong to which ids. The `Corpus` class will also handle the crucial functionality of generating negative samples for training (i.e., randomly-sampled words that were not in the target word's context).\n",
    "\n",
    "Some parts of this class can be completed after you've gotten word2vec up and running, so see the notes below and the details in the homework PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Word2Vec import Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the corpus\n",
    "\n",
    "Now that we have code to turn the text into training data, let's do so. We've provided several files for you to help:\n",
    "\n",
    "* `reviews-word2vec.tiny.txt` -- use this to debug your corpus reader\n",
    "* `reviews-word2vec.med.txt` -- use this to debug/verify the whole word2vec works\n",
    "* `reviews-word2vec.large.txt.gz` -- use this when everything works to generate your vectors for later parts\n",
    "* `reviews-word2vec.HUGE.gz` -- _do not use this_ unless (1) everything works and (2) you really want to test/explore. This file is not needed at all to do your homework.\n",
    "\n",
    "We recommend starting to debug with the first file, as it is small and fast to load (quicker to find bugs). When debugging, we recommend setting the `min_token_freq` argument to 2 so that you can verify that part of the code is working but you still have enough word types left to test the rest.\n",
    "\n",
    "You'll use the remaining files later, where they're described.\n",
    "\n",
    "In the next cell, create your `Corpus`, read in the data, and generate the negative sampling table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data and tokenizing\n",
      "Counting token frequencies\n",
      "Performing minimum thresholding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65523it [00:00, 3542542.74it/s]\n",
      "100%|██████████| 10607824/10607824 [00:10<00:00, 997668.68it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded all data from reviews-word2vec.med.txt; saw 7822050 tokens (65523 unique)\n",
      "Generating sampling table\n"
     ]
    }
   ],
   "source": [
    "rng = RandomNumberGenerator(1000000)\n",
    "rng.set_max_val(1e6)\n",
    "corpus = Corpus(rng)\n",
    "dataset_name = 'reviews-word2vec.med.txt'\n",
    "corpus.load_data(dataset_name, 2)\n",
    "corpus.generate_negative_sampling_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the training data\n",
    "\n",
    "Once we have the corpus ready, we need to generate our training dataset. Each instance in the dataset is a target word and positive and negative examples of contexts words. Given the target word as input, we'll want to predict (or not predict) these positive and negative context words as outputs using our network. Your task here is to create a python `list` of instances. \n",
    "\n",
    "Your final training data should be a list of tuples in the format ([target_word_id], [word_id_1, ...], [predicted_labels]), where each item in the list is a list:\n",
    "1. The first item is a list consisting only of the target word's ID.\n",
    "2. The second item is a list of word ids for both context words and negative samples \n",
    "3. The third item is a list of labels to predicted for each of the word ids in the second list (i.e., `1` for context words and `0` for negative samples). \n",
    "\n",
    "You will feed these tuples into the PyTorch `DatasetLoader` later that will do the converstion to `Tensor` objects. You will need to make sure that all of the lists in each tuple are `np.array` instances and are not plain python lists for this `Tensor` converstion to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7822050it [00:39, 200052.08it/s]\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "num_negative_samples_per_target = 2\n",
    "\n",
    "\n",
    "def generate_training_data():\n",
    "    training_data = []\n",
    "\n",
    "    # Loop through each token in the corpus and generate an instance for each,\n",
    "    # adding it to training_data\n",
    "    for pos, target_id in tqdm(enumerate(corpus.full_token_sequence_as_ids)):\n",
    "        \n",
    "        if target_id == corpus.word_to_index['<UNK>']:\n",
    "            continue\n",
    "\n",
    "        # For exach target word in our dataset, select context words\n",
    "        # within +/- the window size in the token sequence\n",
    "        lower_bound = max(0, pos - window_size)\n",
    "        upper_bound = min(\n",
    "            len(corpus.full_token_sequence_as_ids), pos + window_size + 1)\n",
    "        context_id = []\n",
    "        for i in range(lower_bound, upper_bound):\n",
    "            if i == pos:\n",
    "                continue\n",
    "            context_id.append(corpus.full_token_sequence_as_ids[i])\n",
    "\n",
    "        # For each positive target, we need to select negative examples of\n",
    "        # words that were not in the context. Use the num_negative_samples_per_target\n",
    "        # hyperparameter to generate these, using the generate_negative_samples()\n",
    "        # method from the Corpus class\n",
    "        negative_size = window_size*2*num_negative_samples_per_target + \\\n",
    "            window_size * 2 - len(context_id)\n",
    "        negative_samples = corpus.generate_negative_samples(\n",
    "            target_id, negative_size)\n",
    "        samples = np.concatenate((context_id, negative_samples))\n",
    "        labels = np.concatenate(\n",
    "            (np.ones(len(context_id)), np.zeros(len(negative_samples))))\n",
    "        training_data.append((target_id, samples, labels))\n",
    "\n",
    "        # NOTE: this part might not make sense until later when you do the training\n",
    "        # so feel free to revisit it to see why it happens.\n",
    "        #\n",
    "        # Our training will use batches of instances together (compare that\n",
    "        # with HW1's SGD that used one item at a time). PyTorch will require\n",
    "        # that all instances in a batches have the same size, which creates an issue\n",
    "        # for us here since the target wordss at the very beginning or end of the corpus\n",
    "        # have shorter contexts.\n",
    "        #\n",
    "        # To work around these edge-cases, we need to ensure that each instance has\n",
    "        # the same size, which means it needs to have the same number of positive\n",
    "        # and negative examples. Since we are short on positive examples here (due\n",
    "        # to the edge of the corpus), we can just add more negative samples.\n",
    "        #\n",
    "        # YOUR TASK: determine what is the maximum number of context words (positive\n",
    "        # and negative) for any instance and then, for instances that have fewer than\n",
    "        # this number of context words, add in negative examples.\n",
    "        #\n",
    "        # NOTE: The maximum is fixed, so you can precompute this outside the loop\n",
    "        # ahead of time.\n",
    "    return training_data\n",
    "\n",
    "\n",
    "training_data = generate_training_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the network\n",
    "\n",
    "We'll create a new neural network as a subclass of `nn.Module` like we did in Homework 1. However, _unlike_ the network you built in Homework 1, we do not need to used linear layers to implement word2vec. Instead, we will use PyTorch's `Emedding` class, which maps an index (e.g., a word id in this case) to an embedding. \n",
    "\n",
    "Roughly speaking, word2vec's network makes a prediction by computing the dot product of the target word's embedding and a context word's embedding and then passing this dot product through the sigmoid function ($\\sigma$) to predict the probability that the context word was actually in the context. The homework write-up has lots of details on how this works. Your `forward()` function will have to implement this computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Word2Vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network!\n",
    "\n",
    "Now that you have data in the right format and a neural network designed, it's time to train the network and see if it's all working. The trainin code will look surprisingly similar at times to your pytorch code from Homework 1 since all networks share the same base training setup. However, we'll add a few new elements to get you familiar with more common training techniques. \n",
    "\n",
    "For all steps, be sure to use the hyperparameters values described in the write-up.\n",
    "\n",
    "1. Initialize your optimizer and loss function \n",
    "2. Create your network\n",
    "3. Load your dataset into PyTorch's `DataLoader` class, which will take care of batching and shuffling for us (yay!)\n",
    "4. Create a new `SummaryWriter` to periodically write our running-sum of the loss to a tensorboard\n",
    "5. Train your model \n",
    "\n",
    "Two new elements show up. First, we'll be using `DataLoader` which is going to sample data for us and put it in a batch (and also convert the data to `Tensor` objects. You can iterate over the batches and each iteration will return all the items eventually, one batch at a time (a full epoch's worth).\n",
    "\n",
    "The second new part is using `tensorboard`. As you might have noticed in Homework 1, training neural models can take some time. [TensorBoard](https://www.tensorflow.org/tensorboard/) is a handy web-based view that you can check during training to see how the model is doing. We'll use it here and periodically log a running sum of the loss after a set number of steps. The Homework write up has a plot of what this looks like. We'll be doing something simple here with tensorboard but it will come in handy later as you train larger models (for longer) and may want to visually check if your model is converging. TensorBoard was initially written for another deep learning framework, TensorFlow, but proved so useful it was ported to work in PyTorch too and is [easy to integrate](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html).\n",
    "\n",
    "To start training, we recommend training on the `wiki-bios.10k.txt` dataset. This data is small enough you can get through an epoch in a few minutes (or less) while still being large enough you can test whether the model is learning anything by examining common words. Below this cell we've added a few helper functions that you can use to debug and query your model. In particular, the `get_neighbors()` function is a great way to test: if your model has learned anything, the nearest neighbors for common words should seem reasonable (without having to jump through mental hoops). An easy word to test on the `10k` data is \"january\" which should return month-related words as being most similar.\n",
    "\n",
    "**NOTE**: Since we're training biographies, the text itself will be skewed towards words likely to show up biographices--which isn't necessary like \"regular\" text. You may find that your model has few instances of words you think are common, or that the model learns poor or unusual neighbors for these. When querying the neighbors, it can help to think of which words you think are likely to show up in biographies on Wikipedia and use those as probes to see what the model has learned.\n",
    "\n",
    "Once you're convinced the model is learning, switch to the `med` data and train your model as specified in the PDF. Once trained, save your model using the `save()` function at the end of the notebook. This function records your data in a common format for word2vec vectors and lets you load the vectors into other libraries that have more advanced functionality. In particular, you can use the [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) code in other notebook included to explore the vectors and do simple vector analogies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: 0%|          | 6360/1378872 [00:15<55:16, 413.88it/s]  \n",
    "8: 2%|▏         | 6216/344718 [00:15<13:48, 408.53it/s]  \n",
    "32: 7%|▋         | 6166/86180 [00:15<03:17, 405.75it/s]  \n",
    "64: 14%|█▎        | 5883/43090 [00:14<01:34, 394.75it/s]  \n",
    "128: 29%|██▉       | 6222/21545 [00:15<00:38, 392.87it/s]  \n",
    "256: 52%|█████▏    | 5609/10773 [00:15<00:14, 368.27it/s]  \n",
    "512: 100%|██████████| 5387/5387 [00:15<00:00, 356.52it/s]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzimgong\u001b[0m (\u001b[33mteam-orion\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/zim/UMSI/SI630/Assignments/Homework2/wandb/run-20240305_142006-adu2u0oh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/team-orion/si630_hw2/runs/adu2u0oh' target=\"_blank\">good-flower-27</a></strong> to <a href='https://wandb.ai/team-orion/si630_hw2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/team-orion/si630_hw2' target=\"_blank\">https://wandb.ai/team-orion/si630_hw2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/team-orion/si630_hw2/runs/adu2u0oh' target=\"_blank\">https://wandb.ai/team-orion/si630_hw2/runs/adu2u0oh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121648/121648 [07:35<00:00, 266.94it/s]\n",
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>█▇▆▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>60.73434</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">good-flower-27</strong> at: <a href='https://wandb.ai/team-orion/si630_hw2/runs/adu2u0oh' target=\"_blank\">https://wandb.ai/team-orion/si630_hw2/runs/adu2u0oh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240305_142006-adu2u0oh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Set your training stuff, hyperparameters, models, etc. here\n",
    "batch_size = 64\n",
    "embedding_size = 50\n",
    "lr = 5e-5\n",
    "num_epochs = 1\n",
    "max_steps = 1e6\n",
    "model = Word2Vec(len(corpus.word_to_index), embedding_size).to(device)\n",
    "data_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "# TODO: Initialize weights and biases (wandb) here\n",
    "wandb.init(\n",
    "    project=\"si630_hw2\",\n",
    "    config={\n",
    "        \"learning_rate\": lr,\n",
    "        \"architecture\": \"word2vec\",\n",
    "        \"dataset\": dataset_name,\n",
    "        \"epochs\": num_epochs,\n",
    "    }\n",
    ")\n",
    "\n",
    "# HINT: wrapping the epoch/step loops in nested tqdm calls is a great way\n",
    "# to keep track of how fast things are and how much longer training will take\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    loss_sum = 0\n",
    "\n",
    "    # TODO: use your DataLoader to iterate over the data\n",
    "    for step, data in enumerate(tqdm(data_loader)):\n",
    "\n",
    "        # NOTE: since you created the data as a tuple of three np.array instances,\n",
    "        # these have now been converted to Tensor objects for us\n",
    "        target_ids, context_ids, labels = data\n",
    "        target_ids = target_ids.to(device)\n",
    "        context_ids = context_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # TODO: Fill in all the training details here\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(target_ids, context_ids)\n",
    "        loss = loss_fn(predictions.float(), labels.float())\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        # TODO: Based on the details in the Homework PDF, periodically\n",
    "        # report the running-sum of the loss to Weights & Biases (wandb).\n",
    "        # Be sure to reset the running sum after reporting it.\n",
    "        if step % 100 == 0 and step > 0:\n",
    "            wandb.log({\"loss\": loss_sum})\n",
    "            loss_sum = 0\n",
    "\n",
    "        # TODO: it can be helpful to add some early stopping here after\n",
    "        # a fixed number of steps (e.g., if step > max_steps)\n",
    "        if step > max_steps:\n",
    "            break\n",
    "\n",
    "# once you finish training, it's good practice to switch to eval.\n",
    "model.eval()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify things are working\n",
    "\n",
    "Once you have an initial model trained, try using the following code to query the model for what are the nearest neighbor of a word. This code is intended to help you debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(model, word_to_index, target_word):\n",
    "    \"\"\" \n",
    "    Finds the top 10 most similar words to a target word\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for word, index in tqdm(word_to_index.items(), total=len(word_to_index)):\n",
    "        similarity = compute_cosine_similarity(model, word_to_index, target_word, word)\n",
    "        result = {\"word\": word, \"score\": similarity}\n",
    "        outputs.append(result)\n",
    "\n",
    "    # Sort by highest scores\n",
    "    neighbors = sorted(outputs, key=lambda o: o['score'], reverse=True)\n",
    "    return neighbors[1:11]\n",
    "\n",
    "def compute_cosine_similarity(model, word_to_index, word_one, word_two):\n",
    "    '''\n",
    "    Computes the cosine similarity between the two words\n",
    "    '''\n",
    "    try:\n",
    "        word_one_index = word_to_index[word_one]\n",
    "        word_two_index = word_to_index[word_two]\n",
    "    except KeyError:\n",
    "        return 0\n",
    "\n",
    "    embedding_one = model.target_embeddings(torch.LongTensor([word_one_index]).to(device)).cpu()\n",
    "    embedding_two = model.target_embeddings(torch.LongTensor([word_two_index]).to(device)).cpu()\n",
    "    similarity = 1 - abs(float(cosine(embedding_one.detach().squeeze().numpy(),\n",
    "                                      embedding_two.detach().squeeze().numpy())))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65523/65523 [00:04<00:00, 14575.16it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'highly', 'score': 0.9607359843143927},\n",
       " {'word': 'I', 'score': 0.9559921167030977},\n",
       " {'word': 'm', 'score': 0.9545440313180487},\n",
       " {'word': 'bought', 'score': 0.9543723706362278},\n",
       " {'word': 'would', 'score': 0.9521774294922356},\n",
       " {'word': 'read', 'score': 0.9512196543556726},\n",
       " {'word': 've', 'score': 0.9490245503610805},\n",
       " {'word': 'have', 'score': 0.9443953314992476},\n",
       " {'word': 'be', 'score': 0.9440260320233161},\n",
       " {'word': 'anyone', 'score': 0.9438091503515359}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbors(model, corpus.word_to_index, \"recommend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65523/65523 [00:04<00:00, 14697.80it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'daughter', 'score': 0.9988393331145972},\n",
       " {'word': '6', 'score': 0.9981162856294},\n",
       " {'word': '7', 'score': 0.9977655757959818},\n",
       " {'word': '10', 'score': 0.9975314691113029},\n",
       " {'word': 'day', 'score': 0.9973068436806289},\n",
       " {'word': '8', 'score': 0.9972927310894091},\n",
       " {'word': '9', 'score': 0.9972801498259447},\n",
       " {'word': '12', 'score': 0.9970179092559571},\n",
       " {'word': '1', 'score': 0.996735671645931},\n",
       " {'word': 'husband', 'score': 0.9967149695562113}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_neighbors(model, corpus.word_to_index, \"son\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your vectors for the gensim inspection part!\n",
    "\n",
    "Once you have a fully trained model, save it using the code below. Note that we only save the `target_embeddings` from the model, but you could modify the code if you want to save the context vectors--or even try doing fancier things like saving the concatenation of the two or the average of the two!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(model, corpus, filename):\n",
    "    '''\n",
    "    Saves the model to the specified filename as a gensim KeyedVectors in the\n",
    "    text format so you can load it separately.\n",
    "    '''\n",
    "\n",
    "    # Creates an empty KeyedVectors with our embedding size\n",
    "    kv = KeyedVectors(vector_size=model.embedding_size)        \n",
    "    vectors = []\n",
    "    words = []\n",
    "    # Get the list of words/vectors in a consistent order\n",
    "    for index in trange(model.target_embeddings.num_embeddings):\n",
    "        word = corpus.index_to_word[index]\n",
    "        vectors.append(model.target_embeddings(torch.LongTensor([index]).to(device)).cpu().detach().numpy()[0])\n",
    "        words.append(word)\n",
    "\n",
    "    # Fills the KV object with our data in the right order\n",
    "    kv.add_vectors(words, vectors) \n",
    "    kv.save_word2vec_format(filename, binary=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save your vectors / data for the pytorch classifier in Part 4!\n",
    "\n",
    "We'll be to using these vectors later in Part 4. We want to save them in a format that PyTorch can easily use. In particular you'll need to save the _state dict_ of the embeddings, which captures all of its information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65523/65523 [00:02<00:00, 32235.49it/s]\n"
     ]
    }
   ],
   "source": [
    "save(model, corpus, \"./cache/word2vec_model.kv\")\n",
    "torch.save(model.state_dict(), \"./cache/word2vec_emb.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need the mapping from word to index so we can figure out which embedding to use for different words. Save the `corpus` objects mapping to a file using your preferred format (e.g., pickle or json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(corpus, open(\"./cache/word2vec_corpus.pkl\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
